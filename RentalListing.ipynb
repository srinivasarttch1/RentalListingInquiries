{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from geopy.distance import vincenty\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class RentalListing:\n",
    "    \"\"\" This class performs all the feature engineering and builds the four models (LightGBM, GBM, XGBoost, \n",
    "        RandomForest) that we decided along with thier submission and pickle files \"\"\"\n",
    "        \n",
    "    def __init__(self, trainFile, testFile):\n",
    "        self.trainFile = trainFile\n",
    "        self.testFile = testFile\n",
    "        self.__gbm = GradientBoostingClassifier(min_samples_split = 2300, min_samples_leaf = 60, \n",
    "                     learning_rate = 0.1, n_estimators = 260, max_depth = 9, max_features = 18)\n",
    "        self.__lgb = LGBMClassifier(random_state = 10, objective = 'multiclass', learning_rate = 0.07, \n",
    "                     n_estimators = 280, max_depth = 10, min_child_samples = 22, \n",
    "                     feature_fraction = 0.75, max_bin = 265)\n",
    "        self.__xgb = XGBClassifier(objective = 'multi:softprob', learning_rate = 0.05, \n",
    "                         n_estimators = 1300, max_depth = 3, min_child_weight = 1, subsample = 0.7, \n",
    "                         colsample_bytree = 0.85, reg_alpha = 0.075, gamma = 0.3)\n",
    "        self.__rfc = RandomForestClassifier(n_estimators = 1050, max_depth = 30, min_samples_leaf = 1,\n",
    "                    min_samples_split = 10, max_features = 'sqrt', n_jobs = -1)\n",
    "        \n",
    "        self.train_data = None\n",
    "        self.train_labels = None\n",
    "        self.train_labels_pred = None\n",
    "        self.train_labels_pred_prob = None\n",
    "        \n",
    "        self.test_data = None\n",
    "        self.test_listing_id = None\n",
    "        self.test_labels = None\n",
    "        self.test_labels_pred = None\n",
    "        self.test_labels_pred_prob = None\n",
    "    \n",
    "    # read in the data - where magic happens\n",
    "    def read_data(self):\n",
    "        train_data = pd.read_json(self.trainFile)\n",
    "        train_labels = train_data.interest_level\n",
    "        if (self.testFile is not None):\n",
    "            test_data = pd.read_json(self.testFile)\n",
    "            if ('interest_level' in test_data.columns):\n",
    "                test_labels = test_data.interest_level\n",
    "            else:\n",
    "                test_labels = None\n",
    "        else:\n",
    "            train_data, test_data, train_labels, test_labels = train_test_split(train_data, \n",
    "                            train_data['interest_level'], stratify = train_data['interest_level'], \n",
    "                            random_state = 10, test_size = 0.3)\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.test_listing_id = test_data.listing_id\n",
    "        \n",
    "    # static methods - helper methods for feature engineering\n",
    "    def cap_share(x):\n",
    "        return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "    def distance_park(x):\n",
    "        central_park = (40.7829, -73.9654)\n",
    "        current = (x[0], x[1])\n",
    "        return (vincenty(central_park, current).miles) * 100\n",
    "\n",
    "    def distance_city(x):\n",
    "        city_center = (40.7128, -74.0060)\n",
    "        current = (x[0], x[1])\n",
    "        return (vincenty(city_center, current).miles) * 100\n",
    "\n",
    "    # feature engineering stuff\n",
    "    def basicFeatureEngineering(self):\n",
    "        train_data = self.train_data\n",
    "        test_data = self.test_data\n",
    "        \n",
    "        # number of photos in the listing\n",
    "        train_data[\"num_photos\"] = train_data.photos.apply(len)\n",
    "        test_data[\"num_photos\"] = test_data.photos.apply(len)\n",
    "\n",
    "        # number of features listed\n",
    "        train_data[\"num_features\"] = train_data.features.apply(len)\n",
    "        test_data[\"num_features\"] = test_data.features.apply(len)\n",
    "        \n",
    "        # total rooms\n",
    "        train_data[\"total_rooms\"] = train_data[\"bathrooms\"] + train_data[\"bedrooms\"]\n",
    "        test_data[\"total_rooms\"] = test_data[\"bathrooms\"] + test_data[\"bedrooms\"]\n",
    "        \n",
    "        # price per bedroom\n",
    "        train_data[\"price_bed\"] = train_data[\"price\"] / (train_data[\"bedrooms\"] + 1)\n",
    "        test_data[\"price_bed\"] = test_data[\"price\"] / (test_data[\"bedrooms\"] + 1)\n",
    "        \n",
    "        # date at which listing created - converted to date\n",
    "        train_data[\"created\"] = pd.to_datetime(train_data.created)\n",
    "        test_data[\"created\"] = pd.to_datetime(test_data.created)\n",
    "        \n",
    "        # extracting month\n",
    "        train_data[\"created_month\"] = train_data.created.dt.month\n",
    "        test_data[\"created_month\"] = test_data.created.dt.month\n",
    "        \n",
    "        # extracting day\n",
    "        train_data[\"created_day\"] = train_data.created.dt.day\n",
    "        test_data[\"created_day\"] = test_data.created.dt.day\n",
    "        \n",
    "        # extracting hour\n",
    "        train_data[\"created_hour\"] = train_data.created.dt.hour\n",
    "        test_data[\"created_hour\"] = test_data.created.dt.hour\n",
    "        \n",
    "        # extracting total_days\n",
    "        train_data[\"total_days\"] = (train_data[\"created_month\"] - 4.0) * 30 + train_data[\"created_day\"] + train_data[\"created_hour\"] / 25.0\n",
    "        test_data[\"total_days\"] = (test_data[\"created_month\"] - 4.0) * 30 + test_data[\"created_day\"] + test_data[\"created_hour\"] / 25.0\n",
    "        \n",
    "        # number of words in the description\n",
    "        train_data[\"num_description_words\"] = train_data.description.apply(lambda x: len(x.split(\" \")))\n",
    "        test_data[\"num_description_words\"] = test_data.description.apply(lambda x: len(x.split(\" \")))\n",
    "        \n",
    "        # do capital words in description have a negative effect\n",
    "        train_data['num_cap_share'] = train_data['description'].apply(RentalListing.cap_share)\n",
    "        test_data['num_cap_share'] = test_data['description'].apply(RentalListing.cap_share)\n",
    "        \n",
    "        # number of description lines\n",
    "        train_data['no_lines_desc'] = train_data['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "        test_data['no_lines_desc'] = test_data['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "        \n",
    "        # distance from Cental Park\n",
    "        lat_lon_frames_train = train_data[['latitude','longitude']].copy()\n",
    "        temp_dist1 = lat_lon_frames_train.apply(RentalListing.distance_park, axis = 1)\n",
    "        train_data['distance_park'] = temp_dist1\n",
    "        \n",
    "        lat_lon_frames_test = test_data[['latitude','longitude']].copy()\n",
    "        temp_dist2 = lat_lon_frames_test.apply(RentalListing.distance_park, axis = 1)\n",
    "        test_data['distance_park'] = temp_dist2\n",
    "        \n",
    "        # distance from City Ccenter\n",
    "        temp_dist3 = lat_lon_frames_train.apply(RentalListing.distance_city, axis = 1)\n",
    "        train_data['distance_center'] = temp_dist3\n",
    "        \n",
    "        temp_dist4 = lat_lon_frames_test.apply(RentalListing.distance_city, axis = 1)\n",
    "        test_data['distance_center'] = temp_dist4\n",
    "        \n",
    "        # count of manager id's\n",
    "        manager_id_count_train = train_data.manager_id.value_counts()\n",
    "        train_data[\"manager_count\"] = list(map(lambda x: manager_id_count_train[x], train_data[\"manager_id\"]))\n",
    "        \n",
    "        manager_id_count_test = test_data.manager_id.value_counts()\n",
    "        test_data[\"manager_count\"] = list(map(lambda x: manager_id_count_test[x], test_data[\"manager_id\"]))\n",
    "        \n",
    "        # number of listings with same address\n",
    "        street_count_train = train_data[\"street_address\"].value_counts() \n",
    "        train_data[\"street_count\"] = list(map(lambda x:street_count_train[x], train_data[\"street_address\"]))\n",
    "        \n",
    "        street_count_test = test_data[\"street_address\"].value_counts() \n",
    "        test_data[\"street_count\"] = list(map(lambda x:street_count_test[x], test_data[\"street_address\"]))\n",
    "    \n",
    "        # taking building_count as a feature\n",
    "        building_id_count_train = train_data.building_id.value_counts()\n",
    "        train_data[\"building_count\"] = list(map(lambda x:building_id_count_train[x], train_data[\"building_id\"]))\n",
    "        \n",
    "        building_id_count_test = test_data.building_id.value_counts()\n",
    "        test_data[\"building_count\"] = list(map(lambda x:building_id_count_test[x], test_data[\"building_id\"]))\n",
    "        \n",
    "        # price variable column for k means clustering\n",
    "        train_data[\"price_var\"] = train_data.price\n",
    "        test_data[\"price_var\"] = test_data.price\n",
    "    \n",
    "        # price average on latitude\n",
    "        train_data[\"price_latitude\"] = train_data[\"price\"] / (train_data[\"latitude\"] + 1.0)\n",
    "        test_data[\"price_latitude\"] = test_data[\"price\"] / (test_data[\"latitude\"] + 1.0)\n",
    "        \n",
    "        # rank difference\n",
    "        train_data[\"diff_rank\"] = train_data[\"total_days\"] / (train_data[\"listing_id\"] + 1)\n",
    "        test_data[\"diff_rank\"] = test_data[\"total_days\"] / (test_data[\"listing_id\"] + 1)\n",
    "        \n",
    "        X_vars = ['bathrooms', 'total_rooms', 'latitude', 'longitude', 'price', 'num_photos', \n",
    "                  'num_features', \n",
    "                  'price_bed', 'created_day', 'total_days', 'num_description_words', 'no_lines_desc', \n",
    "                  'description', 'num_cap_share',\n",
    "                  'distance_park', 'distance_center', 'price_latitude', 'diff_rank', 'manager_count', \n",
    "                  'street_count', 'building_count', 'manager_id', 'features', 'price_var']\n",
    "\n",
    "        self.train_data = train_data[X_vars]\n",
    "        self.test_data = test_data[X_vars]\n",
    "        \n",
    "    def addManagerSkill(self):\n",
    "        # computing manager skill \n",
    "        temp = pd.concat([self.train_data[\"manager_id\"], pd.get_dummies(self.train_labels)], axis = 1).groupby(\"manager_id\").mean()\n",
    "        temp.rename(columns = {\"high\": \"manager_id_high_frac\", \"medium\": \"manager_id_medium_frac\", \"low\": \"manager_id_low_frac\"}, inplace = True)\n",
    "        temp['manager_id_count'] = self.train_data.groupby(\"manager_id\").count().iloc[:, 1]\n",
    "        temp['manager_id_skill'] = temp['manager_id_high_frac'] * 2 + temp['manager_id_medium_frac']\n",
    "        mean = temp.loc[temp['manager_id_count'] >= 6, 'manager_id_skill'].mean()\n",
    "        temp.loc[temp['manager_id_count'] < 6, 'manager_id_skill'] = mean\n",
    "        \n",
    "        # adding for training data\n",
    "        self.train_data = pd.merge(left = self.train_data, right = temp, how = 'left', left_on = \"manager_id\", right_index = True)\n",
    "        self.train_data[\"manager_id_skill\"].fillna(mean, inplace = True)\n",
    "        \n",
    "        # adding for testing data\n",
    "        self.test_data = pd.merge(left = self.test_data, right = temp, how = 'left', left_on = \"manager_id\", right_index = True)\n",
    "        self.test_data[\"manager_id_skill\"].fillna(mean, inplace = True)\n",
    "        \n",
    "        # dropping un-necessary columns\n",
    "        self.train_data.drop(columns = ['manager_id', 'manager_id_count', 'manager_id_high_frac', \n",
    "                                'manager_id_medium_frac', 'manager_id_low_frac'], inplace = True)\n",
    "        self.test_data.drop(columns = ['manager_id', 'manager_id_count', 'manager_id_high_frac', \n",
    "                                'manager_id_medium_frac', 'manager_id_low_frac'], inplace = True)\n",
    "    \n",
    "    # preprocessing for feature column\n",
    "    def cleanupListingFeatures(x):\n",
    "        x = x.replace('dryer_in_building', 'dryer')\n",
    "        x = x.replace('dryer_in_unit', 'dryer')\n",
    "        x = x.replace('_dryer', 'dryer')\n",
    "        \n",
    "        x = x.replace('elevator_bldg_', 'elevator')\n",
    "        x = x.replace('_elev', 'elevator')\n",
    "        \n",
    "        x = x.replace('gym_in_building', 'gym')\n",
    "        x = x.replace('fitness_center', 'gym')\n",
    "        x = x.replace('fitness', 'gym')\n",
    "        x = x.replace('health_club', 'gym')\n",
    "        \n",
    "        x = x.replace('hardwood_floors', 'hfloors')\n",
    "        x = x.replace('hardwood', 'hfloors')\n",
    "        \n",
    "        x = x.replace('high_ceilings', 'high_ceiling')\n",
    "        \n",
    "        x = x.replace('in_kitchen_', 'in_kitchen')\n",
    "        \n",
    "        x = x.replace('site_laundry', 'laundry')\n",
    "        x = x.replace('laundry_in_building', 'laundry')\n",
    "        x = x.replace('laundry_in_unit', 'laundry')\n",
    "        x = x.replace('laundry_on_floor', 'laundry')\n",
    "        x = x.replace('laundry_room', 'laundry')\n",
    "        x = x.replace('_lndry_bldg_', 'laundry')\n",
    "        x = x.replace('lndry_bldg_', 'laundry')\n",
    "        x = x.replace('laundry_', 'laundry')\n",
    "        \n",
    "        x = x.replace('fireplaces', 'fireplace')\n",
    "        \n",
    "        x = x.replace('high_speed_internet', 'internet')\n",
    "        x = x.replace('wifi_access', 'internet')\n",
    "        x = x.replace('wifi', 'internet')\n",
    "        x = x.replace('speed_internet', 'internet')\n",
    "        \n",
    "        x = x.replace('high_rise', 'hrise')\n",
    "        x = x.replace('highrise', 'hrise')\n",
    "        x = x.replace('hi_rise', 'hrise')\n",
    "        x = x.replace('high', 'hrise ')\n",
    "        \n",
    "        x = x.replace('_washer', 'washer')\n",
    "        x = x.replace('_dishwasher', 'washer')\n",
    "        x = x.replace('dishwasher', 'washer')\n",
    "        x = x.replace('unit_washer', 'washer')\n",
    "        x = x.replace('washer_in_unit', 'washer')\n",
    "        x = x.replace('washer_', 'washer')\n",
    "        \n",
    "        x = x.replace('wheelchair_ramp', 'wheelchair_access')\n",
    "        \n",
    "        x = x.replace('outdoor_roof_overlookingnew_constructionyork_harbor_and_battery_park', 'roof')\n",
    "        x = x.replace('roof_deck', 'roof')\n",
    "        x = x.replace('roofdeck', 'roof')\n",
    "        x = x.replace('rooftop_deck', 'roof')\n",
    "        x = x.replace('rooftop_terrace', 'roof')\n",
    "        x = x.replace('common_terrace', 'roof')\n",
    "        x = x.replace('common_roof_deck', 'roof')\n",
    "        x = x.replace('terraces_', 'roof')\n",
    "        x = x.replace('terrace', 'roof')\n",
    "        \n",
    "        x = x.replace('cats_allowed', 'pet_ok')\n",
    "        x = x.replace('_pets_ok_', 'pet_ok')\n",
    "        x = x.replace('cats_allowed', 'pet_ok')\n",
    "        x = x.replace('dogs_allowed', 'pet_ok')\n",
    "        x = x.replace('pet_friendly', 'pet_ok')\n",
    "        x = x.replace('pets_allowed', 'pet_ok')\n",
    "        x = x.replace('pets_on_approval', 'pet_ok')\n",
    "        x = x.replace('pets', 'pet_ok ')\n",
    "        \n",
    "        x = x.replace('newly_renovated', 'new_construction')\n",
    "        x = x.replace('_new_', 'new_construction')\n",
    "        \n",
    "        x = x.replace('central_ac', 'ac')\n",
    "        x = x.replace('central_air', 'ac')\n",
    "        x = x.replace('air_conditioning', 'ac')\n",
    "        x = x.replace('central_a', 'ac')\n",
    "        \n",
    "        x = x.replace('parking_available', 'parking')\n",
    "        x = x.replace('parking_space', 'parking')\n",
    "        x = x.replace('site_parking_lot', 'parking')\n",
    "        x = x.replace('site_parking', 'parking')\n",
    "        x = x.replace('parking_lot', 'parking')\n",
    "        x = x.replace('common_parking', 'parking')\n",
    "        \n",
    "        x = x.replace('7_doorman', 'doorman')\n",
    "        x = x.replace('time_doorman', 'doorman')\n",
    "        x = x.replace('ft_doorman', 'doorman')\n",
    "        \n",
    "        x = x.replace('7_concierge', 'concierge')\n",
    "        x = x.replace('concierge_service', 'concierge')\n",
    "        \n",
    "        x = x.replace('site_garage', 'garage')\n",
    "        x = x.replace('full_service_garage', 'garage')\n",
    "        \n",
    "        x = x.replace('residents_garden', 'garden')\n",
    "        x = x.replace('common_garden', 'garden')\n",
    "    \n",
    "        x = x.replace('indoor_pool', 'pool')\n",
    "        x = x.replace('swimming_pool', 'pool')\n",
    "        x = x.replace('outdoor_pool', 'pool')\n",
    "        \n",
    "        x = x.replace('post_war', 'post')\n",
    "        \n",
    "        x = x.replace('prewar', 'pre')\n",
    "        \n",
    "        x = x.replace('valet_services_including_dry_cleaning', 'valet')\n",
    "        x = x.replace('valet_services', 'valet')\n",
    "        x = x.replace('valet_parking', 'valet')\n",
    "    \n",
    "        return x\n",
    "\n",
    "    def addListingFeatures(self):\n",
    "        feature_transform = CountVectorizer(stop_words = 'english', max_features = 50)\n",
    "        \n",
    "        features_column_train = self.train_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
    "        features_column_test = self.test_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
    "        \n",
    "        features_column_train = features_column_train.apply(RentalListing.cleanupListingFeatures)\n",
    "        features_column_test = features_column_test.apply(RentalListing.cleanupListingFeatures)\n",
    "        \n",
    "        # fitting the count vectorizer\n",
    "        feature_transform.fit(list(features_column_train) + list(features_column_test))\n",
    "        \n",
    "        # one hot encoding for the train data\n",
    "        feat_sparse1 = feature_transform.transform(features_column_train)\n",
    "        vocabulary1 = feature_transform.vocabulary_\n",
    "        temp1 = pd.DataFrame([pd.Series(feat_sparse1[i].toarray().ravel()) for i in np.arange(feat_sparse1.shape[0])])\n",
    "        temp1.columns = list(sorted(vocabulary1.keys()))\n",
    "        self.train_data = pd.concat([self.train_data.reset_index(), temp1.reset_index()], axis = 1)\n",
    "        self.train_data.drop(columns = ['index', 'features'], inplace = True)\n",
    "\n",
    "        # one hot encoding for the test data\n",
    "        feat_sparse2 = feature_transform.transform(features_column_test)\n",
    "        vocabulary2 = feature_transform.vocabulary_\n",
    "        temp2 = pd.DataFrame([pd.Series(feat_sparse2[i].toarray().ravel()) for i in np.arange(feat_sparse2.shape[0])])\n",
    "        temp2.columns = list(sorted(vocabulary2.keys()))\n",
    "        self.test_data = pd.concat([self.test_data.reset_index(), temp2.reset_index()], axis = 1)\n",
    "        self.test_data.drop(columns = ['index', 'features'], inplace = True)\n",
    "    \n",
    "    def cleanupListingDescription(x):\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "        # keeping only alphabets and spaces\n",
    "        regex = re.compile('[^a-zA-Z ]')\n",
    "        i = regex.sub(' ', x).lower()\n",
    "        i = i.split(\" \") \n",
    "        i = [stemmer.stem(l) for l in i]\n",
    "        # Keeping words that have length greater than 2\n",
    "        i = \" \".join([l.strip() for l in i if (len(l) > 2) ]) \n",
    "        return i\n",
    "        \n",
    "    def addListingDescription(self):        \n",
    "        # basic cleanup on the description column\n",
    "        train_desc_new = self.train_data.description.apply(lambda x: RentalListing.cleanupListingDescription(x))\n",
    "        test_desc_new = self.test_data.description.apply(lambda x: RentalListing.cleanupListingDescription(x))\n",
    "        \n",
    "        # fit the count vectorizer\n",
    "        desc_transform = CountVectorizer(stop_words = 'english', max_features = 25)\n",
    "        desc_transform.fit(list(train_desc_new) + list(test_desc_new))\n",
    "        \n",
    "        # fit for training data\n",
    "        desc_sparse1 = desc_transform.transform(train_desc_new)\n",
    "        vocabulary1 = desc_transform.vocabulary_\n",
    "        temp1 = pd.DataFrame([pd.Series(desc_sparse1[i].toarray().ravel()) for i in np.arange(desc_sparse1.shape[0])])\n",
    "        temp1.columns = list(sorted(vocabulary1.keys()))\n",
    "        self.train_data = pd.concat([self.train_data.reset_index(), temp1.reset_index()], axis = 1)\n",
    "        self.train_data.drop(columns=['index', 'description'], inplace = True)\n",
    "        \n",
    "        # fit for testing data\n",
    "        desc_sparse2 = desc_transform.transform(test_desc_new)\n",
    "        vocabulary2 = desc_transform.vocabulary_\n",
    "        temp2 = pd.DataFrame([pd.Series(desc_sparse2[i].toarray().ravel()) for i in np.arange(desc_sparse2.shape[0])])\n",
    "        temp2.columns = list(sorted(vocabulary2.keys()))\n",
    "        self.test_data = pd.concat([self.test_data.reset_index(), temp2.reset_index()], axis = 1)\n",
    "        self.test_data.drop(columns=['index', 'description'], inplace = True)\n",
    "        \n",
    "    def addClusterLabels(self):\n",
    "        X = pd.DataFrame()\n",
    "        X['lat'] = self.train_data.latitude\n",
    "        X['long'] = self.train_data.longitude\n",
    "        kmeans = KMeans(n_clusters = 60, random_state = 2301, n_init = 60, max_iter = 500).fit(X)\n",
    "        \n",
    "        # adding cluster labels for training data\n",
    "        self.train_data['label'] = kmeans.labels_\n",
    "        \n",
    "        # inner function\n",
    "        def assignLabel(x):\n",
    "            lat = x.latitude\n",
    "            lon = x.longitude\n",
    "            current = (lat, lon)\n",
    "            min_val = 999999\n",
    "            result = -1\n",
    "            label = 0\n",
    "            for c in kmeans.cluster_centers_:\n",
    "                temp = distance.euclidean(c, current)\n",
    "                if (temp < min_val):\n",
    "                    min_val = temp\n",
    "                    result = label\n",
    "                label = label + 1\n",
    "            return result     \n",
    "        \n",
    "        self.test_data['label'] = self.test_data.apply((lambda x: assignLabel(x)), axis = 1)\n",
    "        \n",
    "    def addPriceVariance(self):\n",
    "        # variance addition\n",
    "        temp_label = self.train_data.groupby(['label'])['price'].median()\n",
    "\n",
    "        # for training data\n",
    "        for i, mean in zip(range(60), temp_label):\n",
    "            cluster = np.where(self.train_data['label'] == i)[0]\n",
    "            self.train_data.price_var[cluster] -= mean\n",
    "\n",
    "        # for testing data\n",
    "        for i, mean in zip(range(60), temp_label):\n",
    "            cluster = np.where(self.test_data['label'] == i)[0]\n",
    "            self.test_data.price_var[cluster] -= mean\n",
    "        \n",
    "    def removeDupNames(self):\n",
    "        df_cols = self.train_data.columns \n",
    "    \n",
    "        seen = set()\n",
    "        un_list = []\n",
    "        for item in df_cols:\n",
    "            fudge = 1\n",
    "            newitem = item\n",
    "            while newitem in seen:\n",
    "                fudge += 1\n",
    "                newitem = \"{}_{}\".format(item, fudge)\n",
    "            un_list = un_list + [newitem]\n",
    "            seen.add(newitem)\n",
    "\n",
    "        # df_cols has unique names\n",
    "        # assigning them back\n",
    "        self.train_data.columns = un_list\n",
    "        self.test_data.columns = un_list\n",
    "        \n",
    "    def featureEngineering(self):\n",
    "        # basic feature engineering\n",
    "        self.basicFeatureEngineering()\n",
    "        \n",
    "        # adding manager skill\n",
    "        self.addManagerSkill()\n",
    "        \n",
    "        # adding feature columns\n",
    "        self.addListingFeatures()\n",
    "        \n",
    "        # adding description columns\n",
    "        self.addListingDescription()\n",
    "        \n",
    "        # adding k means stuff\n",
    "        self.addClusterLabels()\n",
    "        \n",
    "        # add price variance based on the cluster labels\n",
    "        self.addPriceVariance()\n",
    "        \n",
    "        # rename duplicate columns\n",
    "        self.removeDupNames()\n",
    "    \n",
    "    # lightGBM stuff\n",
    "    def trainLightGBM(self):\n",
    "        self.__lgb.fit(self.train_data, self.train_labels)\n",
    "        \n",
    "    def printTestResultsLightGBM(self):\n",
    "        # make predictions - for training set\n",
    "        self.train_labels_pred = self.__lgb.predict(self.train_data)\n",
    "        self.train_labels_pred_prob = self.__lgb.predict_proba(self.train_data)\n",
    "        \n",
    "        # print for train set\n",
    "        print (\"Model Report - Training Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.train_labels, self.train_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.train_labels, self.train_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.train_labels, self.train_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "        # print for test set if labels provided\n",
    "        if (self.test_labels is None):\n",
    "            print (\"\\nTest Data - Labels Missing\")\n",
    "            return\n",
    "        \n",
    "        # make predictions - for test set\n",
    "        self.test_labels_pred = self.__lgb.predict(self.test_data)\n",
    "        self.test_labels_pred_prob = self.__lgb.predict_proba(self.test_data)\n",
    "        \n",
    "        # print for test set\n",
    "        print (\"\\nModel Report - Test Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.test_labels, self.test_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.test_labels, self.test_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.test_labels, self.test_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "    def getSubmissionFileLightGBM(self):\n",
    "        self.test_labels_pred_prob = self.__lgb.predict_proba(self.test_data)\n",
    "        y = self.test_labels_pred_prob\n",
    "        y_df = pd.DataFrame()\n",
    "        y_df[\"listing_id\"] = self.test_listing_id\n",
    "        labels2idx = {label: i for i, label in enumerate(self.__lgb.classes_)}\n",
    "        for label in [\"high\", \"medium\", \"low\"]:\n",
    "            y_df[label] = y[:, labels2idx[label]]\n",
    "        y_df.to_csv(\"submission_light_gbm.csv\", index = False)\n",
    "    \n",
    "    def getPickleFileLightGBM(self):\n",
    "        pickle_out = open(\"model_lgb.pickle\",\"wb\")\n",
    "        pickle.dump(self.__lgb, pickle_out)\n",
    "        pickle_out.close()\n",
    "    \n",
    "    # Gradient Boosting stuff\n",
    "    def trainGBM(self):\n",
    "        self.__gbm.fit(self.train_data, self.train_labels)\n",
    "        \n",
    "    def printTestResultsGBM(self):\n",
    "        # make predictions - for training set\n",
    "        self.train_labels_pred = self.__gbm.predict(self.train_data)\n",
    "        self.train_labels_pred_prob = self.__gbm.predict_proba(self.train_data)\n",
    "        \n",
    "        # print for train set\n",
    "        print (\"Model Report - Training Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.train_labels, self.train_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.train_labels, self.train_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.train_labels, self.train_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "        # print for test set if labels provided\n",
    "        if (self.test_labels is None):\n",
    "            print (\"\\nTest Data - Labels Missing\")\n",
    "            return\n",
    "        \n",
    "        # make predictions - for test set\n",
    "        self.test_labels_pred = self.__gbm.predict(self.test_data)\n",
    "        self.test_labels_pred_prob = self.__gbm.predict_proba(self.test_data)\n",
    "        \n",
    "        # print for test set\n",
    "        print (\"\\nModel Report - Test Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.test_labels, self.test_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.test_labels, self.test_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.test_labels, self.test_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "    def getSubmissionFileGBM(self):\n",
    "        self.test_labels_pred_prob = self.__gbm.predict_proba(self.test_data)\n",
    "        y = self.test_labels_pred_prob\n",
    "        y_df = pd.DataFrame()\n",
    "        y_df[\"listing_id\"] = self.test_listing_id\n",
    "        labels2idx = {label: i for i, label in enumerate(self.__gbm.classes_)}\n",
    "        for label in [\"high\", \"medium\", \"low\"]:\n",
    "            y_df[label] = y[:, labels2idx[label]]\n",
    "        y_df.to_csv(\"submission_gbm.csv\", index = False)\n",
    "    \n",
    "    def getPickleFileGBM(self):\n",
    "        pickle_out = open(\"model_gbm.pickle\",\"wb\")\n",
    "        pickle.dump(self.__gbm, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    # xgboost stuff\n",
    "    def trainXGB(self):\n",
    "        self.__xgb.fit(self.train_data, self.train_labels)\n",
    "        \n",
    "    def printTestResultsXGB(self):\n",
    "        # make predictions - for training set\n",
    "        self.train_labels_pred = self.__xgb.predict(self.train_data)\n",
    "        self.train_labels_pred_prob = self.__xgb.predict_proba(self.train_data)\n",
    "        \n",
    "        # print for train set\n",
    "        print (\"Model Report - Training Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.train_labels, self.train_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.train_labels, self.train_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.train_labels, self.train_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "        # print for test set if labels provided\n",
    "        if (self.test_labels is None):\n",
    "            print (\"\\nTest Data - Labels Missing\")\n",
    "            return\n",
    "        \n",
    "        # make predictions - for test set\n",
    "        self.test_labels_pred = self.__xgb.predict(self.test_data)\n",
    "        self.test_labels_pred_prob = self.__xgb.predict_proba(self.test_data)\n",
    "        \n",
    "        # print for test set\n",
    "        print (\"\\nModel Report - Test Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.test_labels, self.test_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.test_labels, self.test_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.test_labels, self.test_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "    def getSubmissionFileXGB(self):\n",
    "        self.test_labels_pred_prob = self.__xgb.predict_proba(self.test_data)\n",
    "        y = self.test_labels_pred_prob\n",
    "        y_df = pd.DataFrame()\n",
    "        y_df[\"listing_id\"] = self.test_listing_id\n",
    "        labels2idx = {label: i for i, label in enumerate(self.__xgb.classes_)}\n",
    "        for label in [\"high\", \"medium\", \"low\"]:\n",
    "            y_df[label] = y[:, labels2idx[label]]\n",
    "        y_df.to_csv(\"submission_xgb.csv\", index = False)\n",
    "    \n",
    "    def getPickleFileXGB(self):\n",
    "        pickle_out = open(\"model_xgb.pickle\",\"wb\")\n",
    "        pickle.dump(self.__xgb, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    # random forest stuff\n",
    "    def trainRFC(self):\n",
    "        self.__rfc.fit(self.train_data, self.train_labels)\n",
    "        \n",
    "    def printTestResultsRFC(self):\n",
    "        # make predictions - for training set\n",
    "        self.train_labels_pred = self.__rfc.predict(self.train_data)\n",
    "        self.train_labels_pred_prob = self.__rfc.predict_proba(self.train_data)\n",
    "        \n",
    "        # print for train set\n",
    "        print (\"Model Report - Training Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.train_labels, self.train_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.train_labels, self.train_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.train_labels, self.train_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "        # print for test set if labels provided\n",
    "        if (self.test_labels is None):\n",
    "            print (\"\\nTest Data - Labels Missing\")\n",
    "            return\n",
    "        \n",
    "        # make predictions - for test set\n",
    "        self.test_labels_pred = self.__rfc.predict(self.test_data)\n",
    "        self.test_labels_pred_prob = self.__rfc.predict_proba(self.test_data)\n",
    "        \n",
    "        # print for test set\n",
    "        print (\"\\nModel Report - Test Data\")\n",
    "        print (\"Accuracy: \" + str(metrics.accuracy_score(self.test_labels, self.test_labels_pred) * 100.0))        \n",
    "        print (\"Log Loss: \" + str(metrics.log_loss(self.test_labels, self.test_labels_pred_prob)))\n",
    "        print (\"F-Score: \" + str(metrics.f1_score(self.test_labels, self.test_labels_pred, average = \"macro\") * 100.0))\n",
    "        \n",
    "    def getSubmissionFileRFC(self):\n",
    "        self.test_labels_pred_prob = self.__rfc.predict_proba(self.test_data)\n",
    "        y = self.test_labels_pred_prob\n",
    "        y_df = pd.DataFrame()\n",
    "        y_df[\"listing_id\"] = self.test_listing_id\n",
    "        labels2idx = {label: i for i, label in enumerate(self.__rfc.classes_)}\n",
    "        for label in [\"high\", \"medium\", \"low\"]:\n",
    "            y_df[label] = y[:, labels2idx[label]]\n",
    "        y_df.to_csv(\"submission_rfc.csv\", index = False)\n",
    "    \n",
    "    def getPickleFileRFC(self):\n",
    "        pickle_out = open(\"model_rfc.pickle\",\"wb\")\n",
    "        pickle.dump(self.__rfc, pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    def print_test(self):\n",
    "        print (self.train_data.columns)\n",
    "        print (\"------XXXXXXX------\")\n",
    "        print (self.test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4e3fa47b3b46c8ca30d1c6030b2a0d58e9b3380e"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method cap_share() must be called with RentalListing instance as first argument (got unicode instance instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-503faf6872f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRentalListing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-089966610864>\u001b[0m in \u001b[0;36mfeatureEngineering\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# basic feature engineering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicFeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# adding manager skill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-089966610864>\u001b[0m in \u001b[0;36mbasicFeatureEngineering\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# do capital words in description have a negative effect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_cap_share'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRentalListing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcap_share\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_cap_share'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRentalListing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcap_share\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\install_dir\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unbound method cap_share() must be called with RentalListing instance as first argument (got unicode instance instead)"
     ]
    }
   ],
   "source": [
    "# input's\n",
    "train_data_file = \"../input/train.json\"\n",
    "test_data_file = \"../input/test.json\"\n",
    "# test_data_file = None\n",
    "\n",
    "# getting data ready - feature engineering\n",
    "model = RentalListing(train_data_file, test_data_file)\n",
    "model.read_data()\n",
    "model.featureEngineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3fb00970cf481437cc64d12fb0b0b244d201f9e"
   },
   "outputs": [],
   "source": [
    "# model 1 - light gbm\n",
    "model.trainLightGBM()\n",
    "model.printTestResultsLightGBM()\n",
    "model.getSubmissionFileLightGBM()\n",
    "model.getPickleFileLightGBM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "527139c01d75d651790098050f07e2ec1bbd97db"
   },
   "outputs": [],
   "source": [
    "# model 2 - Gradient Boosting Classifier\n",
    "model.trainGBM()\n",
    "model.printTestResultsGBM()\n",
    "model.getSubmissionFileGBM()\n",
    "model.getPickleFileGBM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbb2600496a088bde0c98431895018d9bc552b60"
   },
   "outputs": [],
   "source": [
    "# model 3 - xgboost\n",
    "model.trainXGB()\n",
    "model.printTestResultsXGB()\n",
    "model.getSubmissionFileXGB()\n",
    "model.getPickleFileXGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6459d2b5f3b6cb3ef75e7079525f04b511fee68d"
   },
   "outputs": [],
   "source": [
    "# model 4 - random forest\n",
    "model.trainRFC()\n",
    "model.printTestResultsRFC()\n",
    "model.getSubmissionFileRFC()\n",
    "model.getPickleFileRFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c33affcef5bf756eee5fd96243a019aa9d779c88"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
